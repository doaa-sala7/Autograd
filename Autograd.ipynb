{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defind computional node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class comp_node:\n",
    "    def __init__(self, val, childern = [],op = \"assign\"):\n",
    "        self.val = val\n",
    "        # THE NODE i start backword is the parent and i go throw child\n",
    "        self.childern = childern\n",
    "        self.op = op\n",
    "        self.grad = 0\n",
    "        self.backward_prop=lambda: None\n",
    "    \n",
    "    def __to_comp_node(self, obj):\n",
    "        # check if the other not object from comp_node class---> \n",
    "        # creat instant from comp_node for the value passed (other) \n",
    "        if not isinstance(obj, comp_node):  \n",
    "            obj = comp_node(val = obj)\n",
    "            return obj\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"op:{self.op} | val: {self.val:.4f} | childern: {len(self.childern)} | grad: {self.grad}\"\n",
    "\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # if not isinstance(other, comp_node):  \n",
    "        #     other = comp_node(val = other)\n",
    "        other = self.__to_comp_node(other)\n",
    "        # creat instant to put sub in it, and this node is a parent for childern self, other \n",
    "        out = comp_node(val=self.val - other.val,childern =[self, other], op=\"sub\" ) \n",
    "        def _backward_prop():\n",
    "            self.grad += out.grad* (1)\n",
    "            other.grad += out.grad *(-1)\n",
    "        out.backward_prop=_backward_prop\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def __rsub__ (self, other): # its a rewrite of sub function \n",
    "        # right subtraction not left\n",
    "        other =self. __to_comp_node(other)\n",
    "      \n",
    "        return other-self #i write the operation direcit as i defind the sub operation above \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        out = comp_node(val=self.val + other.val,childern =[self, other], op=\"add\" ) \n",
    "        def _backward_prop():\n",
    "            self.grad += out.grad* (1)\n",
    "            other.grad += out.grad *(1)\n",
    "        out.backward_prop=_backward_prop\n",
    "        return out\n",
    "    \n",
    "    def __radd__ (self, other): # its a rewrite of add function \n",
    "        other =self. __to_comp_node(other)\n",
    "        return other + self #i write the operation direcit as i defind the add operation above \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = self.__to_comp_node(other)\n",
    "        add = comp_node(val=self.val * other.val,childern =[self, other], op=\"mul\" ) \n",
    "        return add\n",
    "    \n",
    "    def __rmul__ (self, other): \n",
    "        other =self. __to_comp_node(other)\n",
    "      \n",
    "        return other * self  \n",
    "\n",
    "    def __pow__(self, exponent):\n",
    "        if not isinstance(exponent,(int,float)):\n",
    "            raise ValueError(\"unsupport type \")\n",
    "        out= comp_node(val=self.val**exponent,childern=[self],op = f\"power{exponent}\") ##M\n",
    "        def _backward_prop():\n",
    "            self.grad += out.grad*(exponent*self.val**(exponent-1))  ### self.val===M\n",
    "        out.backward_prop=_backward_prop\n",
    "        return out\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.val==other.val\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert comp_node(5).val==5   ### assert is a way to check if my code work or no\n",
    "#assert comp_node(7).val==5   ## it not work ---> give me an error\n",
    "\n",
    "assert (comp_node(5) - comp_node(2)).val ==3 \n",
    "assert (comp_node(5) - 2).val ==3\n",
    "assert (3 - comp_node(val=5)).val == -2\n",
    "\n",
    "\n",
    "assert (comp_node(5) + comp_node(2)).val == 7\n",
    "assert (comp_node(5) + 2).val == 7\n",
    "assert (3 + comp_node(val=5)).val == 8\n",
    "\n",
    "assert (comp_node(5) * comp_node(2)).val == 10\n",
    "assert (comp_node(5) * 2).val == 10\n",
    "assert (3 * comp_node(val=5)).val == 15\n",
    "\n",
    "assert (comp_node(val=5)**2).val==25\n",
    "assert (comp_node(val=5)**2)== comp_node(val=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op:assign | val: 5.0000 | childern: 0 | grad: 0\n"
     ]
    }
   ],
   "source": [
    "print(comp_node(val=5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import Random\n",
    "seed= 5\n",
    "data_gen=Random(seed)\n",
    "data_x= [data_gen.uniform(a=0,b=1.) for _ in range (1000)]\n",
    "data_y=[data_gen.uniform(a=0,b=1.)for _ in range (1000)]\n",
    "xp,yp = comp_node(val=0.3),comp_node(val=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_graph(xp,yp,data_x,data_y):\n",
    "    Ix,Iy = xp - data_x, yp- data_y\n",
    "    gx,gy = Ix**2, Iy**2\n",
    "    M = gx + gy\n",
    "    l = M**0.5\n",
    "    return l,[l,M,gx,gy,Ix,Iy,xp,yp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 op:power0.5 | val: 0.6563 | childern: 1 | grad: 1\n",
      "1 op:add | val: 0.4307 | childern: 2 | grad: 0.7618990103238323\n",
      "2 op:power2 | val: 0.1043 | childern: 1 | grad: 0.7618990103238323\n",
      "3 op:power2 | val: 0.3264 | childern: 1 | grad: 0.7618990103238323\n",
      "4 op:sub | val: -0.3229 | childern: 2 | grad: -0.49203696353670395\n",
      "5 op:sub | val: -0.5713 | childern: 2 | grad: -0.8705743084387342\n",
      "6 op:assign | val: 0.3000 | childern: 0 | grad: -0.49203696353670395\n",
      "7 op:assign | val: 0.3000 | childern: 0 | grad: -0.8705743084387342\n"
     ]
    }
   ],
   "source": [
    "loss,rev_topo_order = loss_graph(xp,yp,data_x[0],data_y[0])\n",
    "rev_topo_order[0].grad=1\n",
    "for i, node in enumerate(rev_topo_order):\n",
    "    node.backward_prop()\n",
    "    print(i,node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
